<div class="container">
  <!--Decision tree algorithom-->
  <div>
    <h5>Decision tree algorithom</h5>
    <p>Consider below complex data set diagram we can not
      split this data set one single line, we want to split this
      one, again and again, this is what decision tree algorithm does, decision
      tree algorithm can use both regression and classification cases</p>
    <img src="https://gdcoder.com/content/images/2019/05/Screen-Shot-2019-05-17-at-00.09.26.png"  class="img-fluid" width="500px" height="300px">
    <p>
      Decision tree algorithm work with both classification and regression cases. Let's see how decision tree work
    </p>
    <img src="https://static.javatpoint.com/tutorial/machine-learning/images/decision-tree-classification-algorithm.png"  class="img-fluid" width="500px" height="300px">

  </div>

  <!--Entropy-->
  <div>
    <h5>Entropy</h5>
    <p>
      If we select a decision node the right way we can get the leaf node
      quickly otherwise the decision tree goes much depth and it much takes much time.
      for selecting the right decision node we can use <i>entropy</i>. Entropy measure priority of split.
    </p>
    <img src="https://miro.medium.com/max/2040/1*S6zcbdAzUvIOKBaWBKp9MA.png"  class="img-fluid" width="500px" height="300px">
    <img src="https://i.ytimg.com/vi/tJmhT3oLXCU/maxresdefault.jpg"  class="img-fluid" width="500px" height="300px">
    <p>This is how entropy work with single node and entropy value between 0-1</p>
  </div>

  <!--Information Gain-->
  <div>
    <h5>Information gain</h5>
    <p>This is often used to decide which of the attributes are the most
      relevant, so they tested near the root of the tree.Helps minimize
      computation cycles (and reach decision faster) in decision tree algorithm.
    </p>
    <img src="https://images.slideplayer.com/26/8753862/slides/slide_9.jpg"  class="img-fluid" width="500px" height="300px">
    <p>//explain</p>
  </div>

  <!--Entropy vs Gini Impurity-->
  <div>
    <h5>Entropy vs Gini Impurity</h5>
    <p>
      Both of them do the same task that calculates the priority of split in a decision tree, but most of the time Gini impurity better than
      than entropy
    </p>
    <img src="https://qph.fs.quoracdn.net/main-qimg-3800e86a4f0a8c548f29b025ce45d4d6"  class="img-fluid" width="500px" height="300px">
    <img src="https://abhyast.files.wordpress.com/2015/01/image9.png"  class="img-fluid" width="500px" height="300px">
  </div>

</div>

