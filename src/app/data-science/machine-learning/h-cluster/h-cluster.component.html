<div class="container">
  <!--Hirechical Clustering-->
  <div>
    <h5>Hierarchical Clustering</h5>
    <img class="img-fluid" src="https://cdn-images-1.medium.com/max/800/1*ET8kCcPpr893vNZFs8j4xg.gif">
    <img class="img-fluid" src="https://lh3.googleusercontent.com/proxy/FKqaqJeBygSnr7lFS9KIIvL36EF3omlj-4vi2mVIz_F3-kfY9gUFytz86O7FXrcfHnHYcwwYAbTvLnhRkI6S_2liMnKsNN2QOeDFOdW8HgcoQoKI45daVT9DQTmeW6Z_vzA8zs1SQj008YL7WWX2alRdkxUYjFIBVwJezVHBlcwHtNhhGwqhrtxj0S918C06eyDatWy18z6zu517VQ3NiMyIE8kI8Ymo0zoACg">
    <br>
    <b>Maths Intuition behind Hierarchical Clustering</b><br>
    <img class="img-fluid" width="700px" src="https://46gyn61z4i0t1u1pnq2bbk2e-wpengine.netdna-ssl.com/wp-content/uploads/2018/03/Screen-Shot-2018-03-28-at-11.48.48-am.png">
    <p>
      Here we have 6 different data points ( A, B, C, D, E, F )
      and these are different clusters. this algorithm finds out two nearest clusters.
      first create E, F, and find the distance between these clusters. Now E, F consider
      as one cluster. Next, find out another nearest points A, B, and after calculating the distance between A, B,
      and now A, B are considered as one cluster. In this way, we combined each cluster and represent
      them using a dendrogram. Here y-axis represents the distance between clusters
    </p>
    <p>These points distance measure by using Euclidean distance </p>
    <img class="img-fluid" src="https://bigsnarf.files.wordpress.com/2012/03/distance.jpg?w=584">
    <img class="img-fluid" src="https://ars.els-cdn.com/content/image/3-s2.0-B9780124157811000091-f09-04-9780124157811.jpg">
    <p>Finally, this Algorithm finds the long distance between clusters and make a horizontal line and find how many clusters are here. In the above dendrogram, we can see there are 4 clusters </p>
  </div>
</div>
