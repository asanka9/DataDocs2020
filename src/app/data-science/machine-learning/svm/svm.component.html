<div class="container">

  <!--Support Vector Machine-->
  <div>
    <h5>Support Vector Machine ( SVM )</h5>
    <p>A support vector machine can use both classification and regression cases,
      but most of the cases this algorithm used for classification use cases
      fist we consider how this algorithm works with classification</p>

      <img src="https://lh3.googleusercontent.com/proxy/DwuB02CUaLuSVXyRQbi0nzgilQ6Gi537pxDE3-Zo0OdqaE8xrrV3Bp0-cWaKOW26NQ0apMVGEFy-lpXIiB5M9GBwqH3wwyMkEKiRILGFD-HeyOqeVcyCgJQ"  class="img-fluid" width="500px" height="300px">
      <p>This centerline is also called Hyperplane not only
        that SVM also creates two margin lines with hyperplane.
        These two margin lines easily separable data points and these
        margin lines passing through one of the positive or negative points and these lines parallel
        for hyperplane (These data points are called Support Vectors ). This distance called
        as marginal distance. Any point that will be below the hyperplane that will be considered as a
        negative point and any point above the hyperplane consider as a positive point
      </p>
      <img src="https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTEKF40jaDVN0VOsalEKJQPHSq9xLFazdg60w&usqp=CAU"  class="img-fluid" width="500px" height="300px">
      <p>SVM make different Hyperplane but it is only considered best hyperplane
        which has maximum marginal distance, but theses technique apply for linear
        separable points what if data set will nonlinearly separable </p>
      <b>With Non-Linearly Separable</b><br><br>
      <img src="https://nlp.stanford.edu/IR-book/html/htmledition/img1331.png"  class="img-fluid" width="500px" height="300px"><br>
      <br>
      <p>we are
        presented with data that looks like this
        where it looks almost impossible to use
        a single line to separate the two
        classes we can use a function to
        transform our data into high dimensional
        space so you can see over here we go
        from one dimensional to two dimensional
        space we can apply a simple polynomial
        function to get a parabola and now you
        can easily see how we can draw our
        hyperplane</p><br>
      <img src="https://www.researchgate.net/profile/Arash_Saeidpour/publication/303469788/figure/fig4/AS:668456450019348@1536383845662/SVM-classification-for-non-linearly-separable-data-points.jpg"  class="img-fluid" width="600px" height="300px">
      <img src="https://www.bogotobogo.com/python/scikit-learn/images/svm2/Non-linear-boundaries3.png"  class="img-fluid" width="500px" height="300px">

      <p>In these cases, SVM tries to make Law dimension to High dimension using SVM kernels </p>
      <img src="https://www.saedsayad.com/images/SVR_1.png"  class="img-fluid" width="500px" height="300px">
      <p>//EXPLAIN</p>

  </div>

  <!--Maths Intuition Behind Support Vector Machine-->
  <div>
    <h5>Maths Intuition Behind Support Vector Machine</h5>
    <img src="https://i.ytimg.com/vi/IEOgRGh7x4g/maxresdefault.jpg"  class="img-fluid" width="500px" height="300px">
    <p>In Logistic Regression tries to make Hyperplane with
      2-dimensional graph but one thing is getting added in SVM it
      is the marginal distance </p>
    <p>//EXPLAIN</p>
  </div>
  <!--What Are the Assumptions-->
  <div>
    <h5>What Are the Basic Assumption?</h5>
    <p>There are no such assumptions</p>
  </div>

  <!--Advantages-->
  <div>
    <h5>Advantages</h5>
    <ul>
      <li>SVM is more effective in high dimensional spaces.</li>
      <li>SVM is relatively memory efficient.</li>
      <li>SVMâ€™s are very good when we have no idea on the data.</li>
      <li>Works well with even unstructured and semi structured data like text, Images and trees.</li>
      <li>The kernel trick is real strength of SVM. With an appropriate kernel function, we can solve any complex problem.</li>
      <li>SVM models have generalization in practice, the risk of over-fitting is less in SVM.</li>
    </ul>
  </div>

  <div>
    <h5>Disadvantages</h5>
    <ul>
      <li>More Training Time is required for larger dataset</li>
      <li>It is difficult to choose a good kernel function</li>
      <li>The SVM hyper parameters are Cost -C and gamma. It is not that easy to fine-tune these hyper-parameters. It is hard to visualize their impact</li>
    </ul>
  </div>

  <div>
    <h5>Types of Problems it can solve(Supervised)</h5>
    <ul>
      <li>Regression</li>
      <li>Classification</li>
    </ul>
  </div>

  <div>
    <h5>Whether Feature Scaling is required?</h5>
    <p>Yes</p>
  </div>

  <div>
    <h5>Overfitting And Underfitting</h5>
    <p>In SVM, to avoid overfitting, we choose a Soft Margin,
      instead of a Hard one i.e. we let some data points enter our margin
      intentionally (but we still penalize it) so that our classifier don't overfit on our training sample</p>
  </div>

  <div>
    <h5>Different Problem statement you can solve using Naive Baye's</h5>
    <ul>
      <li>We can use SVM with every ANN usecases</li>
      <li>Intrusion Detection</li>
      <li>Handwriting Recognition</li>
    </ul>
  </div>

  <div>
    <h5>Performance Metrics</h5>
    <ul>
      <li>Classification</li>
      <ul>
        <li>Confusion Matrix</li>
        <li>Precision,Recall, F1 score</li>
      </ul>
      <li>Regression</li>
      <ul>
        <li>R2,Adjusted R2</li>
        <li>MSE,RMSE,MAE</li>
      </ul>
    </ul>
  </div>

</div>
