<div class="container">

  <!--Why Text Preprocessing Techniques need for NLP-->
  <div>
    <h5>Why Text Preprocessing Techniques need for NLP</h5>
    <p>suppose we have text data, we can not directly pass text data to our
      model we need to convert that into some numerical format therefore we need
      some text preprocessing techniques for NLP </p>
    <b>Text Preprocessing Techniques use in NLP</b>
    <ul>
      <li>Stemming</li>
      <li>Lemmatization</li>
      <li>Stop Words</li>
      <li>Tf-IDF</li>
      <li>Bag Of  Words</li>
    </ul>
  </div>

  <!--Tokenization-->
  <div>
    <h5>Tokenization</h5>
    <img src="https://media.springernature.com/original/springer-static/image/chp%3A10.1007%2F978-3-030-34614-0_3/MediaObjects/481454_1_En_3_Fig2_HTML.png" class="img-fluid" width="500px" height="300px">
    <img src="https://miro.medium.com/max/2414/1*UhfwmhMN9sdfcWIbO5_tGg.jpeg"  class="img-fluid" width="500px" height="300px">
    <p>
      Tokenization means that we will be taking paragraphs converting those paragraph into sentences  and words
    </p>
  </div>

  <!--Stemming &  Lematization-->
  <div>
    <h5>Stemming &  Lematization</h5>
    <img src="https://miro.medium.com/max/2050/1*ES5bt7IoInIq2YioQp2zcQ.png"  class="img-fluid" width="500px" height="300px"><br>
    <b>Difference between Stemming & Lematization</b>
    <ul>
      <li>
        <p>Both do the same task but Lemmatization convert words to a
          meaningful manner,in the above example, <i>'chang'</i> do not have meaningful meaning but
          <i>'change'</i> is meaningful that's what lemmatization do  </p>
      </li>
      <li>
        <p>
          Lemmatization usually takes a lot of times,
          because come to meaningful words it needs some process,
          but Stemming does not  take that much time because it is
          found the only base word
        </p>
      </li>
    </ul>
    <b>What are the applications that use Stemming?</b>
    <ul>
      <li>
        Sentement Analysis
      </li>
    </ul>
    <b>What are the applications that use Lemmatization?</b>

    <ul>
      <li>
        Chat Bot
      </li>
      <li>
        QA Applications
      </li>
    </ul>
  </div>

  <!--Lowering the Sentences-->
  <div>
    <h5>Lowering the Sentences</h5>
    <p>If we don not lawer the sentences same word may in ????????</p>
  </div>

  <!--Stop Words-->
  <div>
    <h5>Stop Words</h5>
    <img src="https://miro.medium.com/max/1930/1*-g1tnFOcAUOKWOg4dBKS1w.png"  class="img-fluid" width="700px" height="300px">
    <p>//NEE EXPLAIN</p>
  </div>

    <!--Bag Of Words-->
    <div>
      <h5>Bag Of Words</h5>
      <p>After we applying Stemming or Lemmatization and stop words
        in our text data we can be applying bagofwords, from using this preprocessing
        technique we can convert our text data to the numerical representation</p>
      <b>Types of Bag Of Words</b>
      <ul>
        <li>Binary Bag of words</li>
        <li>Normal Bag of words</li>
      </ul>
      <img src="https://res.cloudinary.com/practicaldev/image/fetch/s--qveZ_g7d--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://raw.githubusercontent.com/cassieview/intro-nlp-wine-reviews/master/imgs/vectorchart.PNG"  class="img-fluid" width="500px" height="300px"><br>
      <i>Binary bag of words</i><br><br>
      <img src="https://miro.medium.com/max/1536/1*3IACMnNpwVlCl8kSTJocPA.png"   class="img-fluid" width="500px" height="300px"><br>
      <i>Normal bag of words</i><br><br>

      <b>Disadvantege of Bag of Words</b>
      <p>We can not find what are the important words that are most
        impacted by the output, for avoiding this one we can use other
        techniques like TF-IDF  vectorizer </p>
    </div>


    <!--TF-IDF-->
    <div>
      <h5>TF-IDF (Term Frequencyâ€“Inverse Document Frequency)</h5>
      <img src="https://miro.medium.com/fit/c/1838/551/1*V9ac4hLVyms79jl65Ym_Bw.jpeg"  class="img-fluid" width="500px" height="300px"><br>
      <b>https://www.youtube.com/watch?v=D2V1okCEsiE&list=PLZoTAELRMXVMdJ5sqbCK2LiM0HhQVWNzm&index=8</b>
      <p>IMAGE</p>
    </div>

      <!--What are the disadvantages of TF-IDF and BagOfWords?-->
  <div>
    <h5>What are the disadvantages of TF-IDF and BagOfWords?</h5>
    <P>Both BOW and TF-IDF approach <i>semantic information</i> is not
      stored, TF-IDF only gives importance to uncommon words, and there
      is a chance of overfitting.we can use Word2Vec to overcome this problem
    </P>
  </div>

  <!--Word2Vec-->
  <div>
    <h5>Word2Vec</h5>
    <img src="https://miro.medium.com/max/3902/1*hELlVp9hmZbDZVFstS61pg.png" class="img-fluid" width="500px" height="300px">
    <p>in this specific model, each word is basically represented as a
      vector of 32 or more dimensions instead of a single number. Here the
      semantic information and relationships between different words are also preserved </p>
    <img src="https://developers.google.com/machine-learning/crash-course/images/linear-relationships.svg"  class="img-fluid" width="500px" height="300px"><br>
    <b>Steps to Create Word2Vec</b>
    <ul>
      <li>Tokenization of the sentences</li>
      <li>Create Histograms</li>
      <li>Take most frequent words</li>
      <li>Create a matrix with all the unique words, it also represents the occurrence relation between the words</li>
    </ul>
  </div>

</div>
